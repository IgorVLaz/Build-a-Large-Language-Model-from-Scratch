{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "860248c1-efae-4b9e-9629-a9a261981bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gpt2_model import GPTModel, generate_text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b449e23a-7670-4aa6-b0e9-fb69416d44a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6175dec8-5b7b-4a19-a119-e93ef973bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be72f82-02d5-44d1-9ea7-678afc081f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735bc961-e081-492f-af8b-f4f5e0f0d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f23bcf2-104e-4db8-bf6a-a2fa065b912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e543ab0-203a-4a5d-9ecf-3248ed2df2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1cce80-061c-456a-8086-856d94c26696",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[16833, 3626, 6100], [40, 1107, 588]]  # [\"every effort moves\",\n",
    ")  #   \"I really like\"]\n",
    "\n",
    "targets = torch.tensor(\n",
    "    [[3626, 6100, 345], [1107, 588, 11311]]  # [\" effort moves you\",\n",
    ")  # \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf738008-7ca0-4aba-b2da-8d3c2df4f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd621ab7-c8dc-4f19-abc4-26e3684bee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509012f9-9d44-4578-9263-19ea9324cc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\" f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57aa31e5-b04e-459c-ac55-a8aa42b80aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc6b818f-9948-49fa-84d8-b154f0d340c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f1d755-963e-4868-a01e-082b94ab2c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c66128fd-7da8-459b-b2b2-0c217cbbca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5644e85-ead2-42c3-9999-17abb340dd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e479e3-3183-47b9-b80d-dabd9c1d6699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff48a6e2-93fd-4f40-a67d-f35402d3d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1615d2e7-18ea-462a-a856-e2cd0d0e67d3",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4426f6f5-5a0f-4574-bc98-ed50338042c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafc14a9-9f86-4e67-af89-44120ffebce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b7dbd4-f3c8-4acb-8b40-37735260db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3130177-08ff-45be-8198-3a23f6138357",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a3e3975-bf9a-4abb-af83-60326a6cbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_len=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_len=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7640c7c-9c22-4415-8e9e-73055aae050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40c1651f-f9ba-4437-ac05-00ffaa2c2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e476c4a9-f398-4d89-8577-6acef0b4f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd6254cf-0e48-4132-b67e-cf3b4e163ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987582948472765\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14dee17d-7046-44c8-8b24-6403d554c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c27720e-0092-48d5-b617-078884de7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a5cefbb-d533-4282-8c26-3dc6e53ca790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer,\n",
    "):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "                generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fc0467f-f248-43cd-b6d2-7f655ae4deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.817, Val loss 9.928\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the the,,,,,,,,,,,,,,,,,,,\n",
      "Ep 1 (Step 000005): Train loss 8.067, Val loss 8.334\n",
      "Every effort moves you, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 2 (Step 000010): Train loss 6.624, Val loss 7.051\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000015): Train loss 6.049, Val loss 6.603\n",
      "Every effort moves you, the, the the the, the, the, the the, the, the, the the, the the, the the, the, the, the the the, the, the the, the the, the the, the, the,\n",
      "Ep 3 (Step 000020): Train loss 5.549, Val loss 6.501\n",
      "Every effort moves you, and, and the a, and the--.                 \"I, and the of the, and the the's, and, and the of the\", and\n",
      "Ep 3 (Step 000025): Train loss 5.413, Val loss 6.366\n",
      "Every effort moves you, and I had the of the of the to the of the of the of the of the of the of the Stroud, and, and I had the the of the of the of the Stroud.        \n",
      "Ep 4 (Step 000030): Train loss 4.912, Val loss 6.283\n",
      "Every effort moves you a \" to the picture the picture. I was the picture. I was a. I was--I--I                           \n",
      "Ep 4 (Step 000035): Train loss 4.665, Val loss 6.310\n",
      "Every effort moves you of the picture.                        \"I\"IHe the picture\"I\"I           \n",
      "Ep 5 (Step 000040): Train loss 3.966, Val loss 6.152\n",
      "Every effort moves you, I was, and I was, I was, I was, I was.     \"I was, and to have to, and I was, and. \"I to have to have to have to have, I\n",
      "Ep 6 (Step 000045): Train loss 3.626, Val loss 6.197\n",
      "Every effort moves you know the        \"Oh a little a little.           \"Oh, and the fact a little the donkey. \"I looked a little the room, and in\n",
      "Ep 6 (Step 000050): Train loss 3.061, Val loss 6.123\n",
      "Every effort moves you know the \"I was a little the to the fact-room.             \"I turned back the fact-c.   \"I looked and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.943, Val loss 6.154\n",
      "Every effort moves you know the picture to see it was not to see the fact with a little a little to have to see it.  \"Oh, and I had been his head to have my elbow and I had the donkey, and in the hour. \n",
      "Ep 7 (Step 000060): Train loss 2.211, Val loss 6.121\n",
      "Every effort moves you know the picture.  I glanced after him, and I was one of the house.\"      \"I didn't say, and I was a little. I was his own the donkey, and the room, I was\n",
      "Ep 8 (Step 000065): Train loss 1.770, Val loss 6.139\n",
      "Every effort moves you know,\" was one of the picture for nothing--I had a little of a and he was no great, I felt to have to see a smile behind his pictures.                \n",
      "Ep 8 (Step 000070): Train loss 1.462, Val loss 6.223\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word. Gisburn's an awful simpleton, and Mrs. Gisburn's an them at my elbow and as he had been the \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.132, Val loss 6.248\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. Gisburn's it was no great, the fact, had been through--it was back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 9 (Step 000080): Train loss 0.855, Val loss 6.281\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  \"Oh, and I remember getting off a prodigious phrase about the honour being _mine_--because he didn't want\n",
      "Ep 10 (Step 000085): Train loss 0.625, Val loss 6.395\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Запуск обучения\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62c79f33-d9c9-4b1d-9cce-992d8a762bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2601dfe5-d55d-4171-bc1a-47226d21a53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48218455-b7d6-49ad-8194-e8a7a49a70f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af8f0d-d0a1-492c-bd23-3d59d810bb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "346b98a2-9225-4e33-88c4-c28ff758585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None\n",
    "):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49d4bc34-5046-45d1-a769-85fb22ff43ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to happen a little wild--I was such a good fellow enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4,\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfe64880-2f5b-416d-adae-85395be8b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_opimizer.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bc35a39-1ff0-467d-aaed-42a84b5d6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_opimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32940669-fa50-4a56-af02-ffde86d6c000",
   "metadata": {},
   "source": [
    "## Loading GPT2 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c05d4b8c-b3e2-49b4-b87e-bed00b995069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0da02f3d-8d61-4ef3-85fc-2d9efdff6f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x17584714410>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f2cbc0a-1961-43bd-bf89-cf8007655bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Игорь\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "checkpoint: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 74.2kiB/s]\n",
      "encoder.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 853kiB/s]\n",
      "hparams.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 116kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|████████████████████████████████████████████████████████████████████████████████| 498M/498M [02:15<00:00, 3.68MiB/s]\n",
      "model.ckpt.index: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 6.54MiB/s]\n",
      "model.ckpt.meta: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 471k/471k [00:00<00:00, 532kiB/s]\n",
      "vocab.bpe: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 537kiB/s]\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f31edaae-60d6-4e8c-be35-cfd2a7b8efd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07a912cd-7f30-4a39-831c-598f69b28e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ecfa30db-d790-4b80-b780-b30140337e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8751eecc-21d6-4c40-bd56-a294398588fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d31aa6e2-e7d1-4cfc-aa6c-fbac35a718a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f75e1e98-5b7a-432f-9768-bb67c6e9c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T\n",
    "        )\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T,\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"],\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a68d101-ec1c-4afb-bfa0-6164c95f53cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45f773f6-87a9-430f-83b6-fd9ee9c586d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Describe me the sky as blue as water, the same air as fire. It was not only sky blue— it smelled as if it had gone to the surface. My face contorts on the stone floor under me. Then, my hair is tinged with flames.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Describe me the sky\", tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5,\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
